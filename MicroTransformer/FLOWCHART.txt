МИКРОТРАНСФОРМЕР: ПОЛНАЯ ДИАГРАММА ПРОЦЕССА
==========================================

┌─────────────────────────────────────────────────────────────────────────┐
│                           MicroTransformer                              │
│               "От текста на входе до текста на выходе"                   │
└─────────────────────────────────────────────────────────────────────────┘

ФАЗА 1: ВВОД ДАННЫХ
===================
┌─────────────────┐    ┌─────────────────┐    ┌─────────────────┐
│   ВХОДНОЙ ТЕКСТ  │ -> │   ТОКЕНИЗАТОР    │ -> │   ЭМБЕДДИНГИ    │
│                 │    │                 │    │   (d_model=5)   │
│ "амба ходи"     │    │ амба=12,        │    │ [emb_амба]      │
│                 │    │ ходи=113        │    │ [emb_ходи]      │
└─────────────────┘    └─────────────────┘    └─────────────────┘

ФАЗА 2: ПОДГОТОВКА К ОБРАБОТКЕ
==============================
┌─────────────────┐    ┌─────────────────┐
│  ПОЗИЦИОННОЕ    │ -> │   МАСКА ВНИМ.    │
│   КОДИРОВАНИЕ   │    │   (Causal)      │
│ [emb + pos]     │    │ [triu_mask]     │
└─────────────────┘    └─────────────────┘

ФАЗА 3: ТРАНСФОРМЕРНАЯ ОБРАБОТКА
================================
┌─────────────────────────────────────────────────────────────────────────┐
│                         ТРАНСФОРМЕРНЫЕ СЛОИ (2)                        │
├─────────────────────────────────────────────────────────────────────────┤
│ ┌─────────────────┐ ┌─────────────────┐ ┌─────────────────┐ ┌─────────┐ │
│ │   Self-Attn     │ │ Layer Norm      │ │ Feed-Forward    │ │  +Norm  │ │
│ │                 │ │                 │ │ Linear(5→20)    │ │         │ │
│ │ Q,K,V from x    │ │ x + attn        │ │ → ReLU →        │ │ x + ff  │ │
│ │                 │ │                 │ │ Linear(20→5)    │ │         │ │
│ └─────────────────┘ └─────────────────┘ └─────────────────┘ └─────────┘ │
└─────────────────────────────────────────────────────────────────────────┘

ФАЗА 4: ВЫХОДНЫЕ ЛОГИТЫ
=======================
┌─────────────────┐    ┌─────────────────┐
│ ТРАНСФОРМЕР     │ -> │  ЛИНЕЙНЫЙ СЛОЙ  │
│ ВЫХОД (5-dim)   │    │ (5 → 240)       │
└─────────────────┘    └─────────────────┘

ФАЗА 5: ГЕНЕРАЦИЯ ТЕКСТА
========================
┌─────────────────────────────────────────────────────────────────────────┐
│                      АВТОРЕГРЕССИВНАЯ ГЕНЕРАЦИЯ                        │
├─────────────────────────────────────────────────────────────────────────┤
│ START → токен1 → токен2 → токен3 → ... → END                          │
│  │        │        │        │                                          │
│  ▼        ▼        ▼        ▼                                          │
│ [1] → [1,12] → [1,12,113] → [1,12,113,176] → ...                      │
│ logits  logits   logits    logits                                     │
│ sample  sample   sample    sample                                     │
└─────────────────────────────────────────────────────────────────────────┘

ФАЗА 6: ДЕТОКЕНИЗАЦИЯ
=====================
┌─────────────────┐    ┌─────────────────┐
│   ТОКЕНЫ        │ -> │  ДЕТОКЕНИЗАТОР  │
│                 │    │                 │
│ 12,113,176,2    │    │ амба, ходи,     │
│                 │    │ тихо, <end>     │
└─────────────────┘    └─────────────────┘

ФАЗА 7: ВЫХОДНОЙ ТЕКСТ
======================
┌─────────────────┐
│  "амба ходи     │
│   тихо"         │
└─────────────────┘

РАЗМЕРЫ ТЕНЗОРОВ (batch_size=32, seq_len=20)
============================================
┌─────────────────────────────────────────────────────────────────────────┐
│ ЭТАП              │ РАЗМЕР ТЕНЗОРА          │ ОПИСАНИЕ                    │
├───────────────────┼─────────────────────────┼─────────────────────────────┤
│ Токенизация       │ (32, 20)               │ Целые числа 0-239           │
│ Эмбеддинги        │ (32, 20, 5)            │ 5-мерные векторы            │
│ Позиц. кодиров.   │ (32, 20, 5)            │ + позиционная информация    │
│ Внимание          │ (32, 20, 5)            │ Контекстные представления   │
│ Feed-Forward      │ (32, 20, 5)            │ Обработанные признаки       │
│ Выход             │ (32, 20, 239)          │ Логиты для каждого токена   │
└─────────────────────────────────────────────────────────────────────────┘

КЛЮЧЕВЫЕ ПАРАМЕТРЫ
==================
• Размер словаря: 240 токенов
• Размерность эмбеддингов: 5
• Количество голов внимания: 1
• Количество слоев: 2
• Размер FFN: 20
• Максимальная длина: 20
• Температура семплинга: 0.7

ФОРМУЛЫ
=======
1. Attention: softmax(Q*K^T / sqrt(d_model)) * V
2. Positional Encoding: PE(pos,2i) = sin(pos/10000^(2i/d_model))
3. Positional Encoding: PE(pos,2i+1) = cos(pos/10000^(2i/d_model))
4. Loss: CrossEntropy(logits, targets, ignore_index=0)

ПОЛНЫЙ ПРОЦЕСС (ОБУЧЕНИЕ)
=========================
Входной текст → Токенизация → Эмбеддинги → PosEnc → Transformer →
Выходные логиты → Loss с целевыми токенами → Backprop → Update

ПОЛНЫЙ ПРОЦЕСС (ГЕНЕРАЦИЯ)
==========================
START токен → Transformer → Семплинг токена → Добавление к последовательности →
Повторять до END или max_length → Детокенизация → Выходной текст

Эта диаграмма показывает полный пайплайн MicroTransformer от входного текста
до выходного, включая все этапы обработки, размеры тензоров и ключевые формулы.
