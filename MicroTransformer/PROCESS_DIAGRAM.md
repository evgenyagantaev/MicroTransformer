# Диаграмма Процесса MicroTransformer

## Общая Архитектура

```
┌─────────────────────────────────────────────────────────────────────────┐
│                           MicroTransformer                              │
├─────────────────────────────────────────────────────────────────────────┤
│  Вход: "амба ходи"  →  Токенизация  →  Эмбеддинги  →  Трансформер  →  Генерация  │
│  Выход: "амба ходи тихо"                                                │
└─────────────────────────────────────────────────────────────────────────┘
```

## Детальный Разбор По Этапам

### Этап 1: Токенизация
```
Входной текст: "амба ходи"
              ↓
Токенизатор (использует словарь из 240 токенов)
              ↓
Токены: [1, 12, 113, 2]  # [START, амба(12), ходи(113), END]
              ↓
Тензор: tensor([[1, 12, 113, 2, 0, 0, ...]])  # Паддинг до max_length=20
```

### Этап 2: Эмбеддинги
```
Токены: [1, 12, 113, 2, 0, 0, ...]
              ↓
Embedding Layer (vocab_size=240, d_model=5)
              ↓
Векторы: [[emb_start], [emb_амба], [emb_ходи], [emb_end], [emb_pad], ...]
         Размер: (1, 20, 5)
```

### Этап 3: Позиционное Кодирование
```
Эмбеддинги: [[e1], [e2], [e3], [e4], [e5], ...]
              ↓
Positional Encoding (добавляет позиционную информацию)
              ↓
Результат: [[e1+pos1], [e2+pos2], [e3+pos3], [e4+pos4], [e5+pos5], ...]
```

### Этап 4: Трансформерные Слои (2 слоя)

#### Слой 1: Self-Attention
```
Вход: [[x1], [x2], [x3], ...]  # (seq_len, d_model)
              ↓
Multi-Head Attention (num_heads=1)
  - Query = x * Wq  # (seq_len, d_model)
  - Key = x * Wk    # (seq_len, d_model)
  - Value = x * Wv  # (seq_len, d_model)
              ↓
Attention Scores: Q * K^T / sqrt(d_model)
              ↓
Softmax + Dropout
              ↓
Weighted Sum: Attention * Value
              ↓
Результат: [attn1, attn2, attn3, ...]
```

#### Слой 2: Feed-Forward
```
Attention Output: [a1, a2, a3, ...]
              ↓
Layer Normalization
              ↓
Feed-Forward: Linear(5→20) → ReLU → Linear(20→5)
              ↓
Add & Norm (Residual Connection)
              ↓
Результат: [ff1, ff2, ff3, ...]
```

### Этап 5: Выходные Логиты
```
Трансформер Выход: [h1, h2, h3, ...]  # (seq_len, d_model)
              ↓
Linear Layer (d_model=5 → vocab_size=240)
              ↓
Логиты: [l1, l2, l3, ..., l240]  # (seq_len, vocab_size)
```

### Этап 6: Авторегрессивная Генерация
```
Итерация 1:
  Вход: [1]  # START token
  Выход: логиты для позиции 1
  Семплинг: выбираем токен 12 ("амба")
  Новый вход: [1, 12]
              ↓
Итерация 2:
  Вход: [1, 12]
  Выход: логиты для позиции 2
  Семплинг: выбираем токен 113 ("ходи")
  Новый вход: [1, 12, 113]
              ↓
... продолжается до END token или max_length
```

### Этап 7: Детокенизация
```
Токены: [1, 12, 113, 176, 2]  # [START, амба, ходи, тихо, END]
              ↓
Детокенизатор: 12→"амба", 113→"ходи", 176→"тихо"
              ↓
Выходной текст: "амба ходи тихо"
```

## Визуализация Тензорных Размеров

### Обучение:
```
Batch Size: 32
Sequence Length: 20
Embedding Dimension: 5
Vocabulary Size: 240

┌─────────────┐
│ Input       │  (32, 20)          # Токены
├─────────────┤
│ Embedding   │  (32, 20, 5)       # Эмбеддинги
├─────────────┤
│ Pos Encoding│  (32, 20, 5)       # + позиционное кодирование
├─────────────┤
│ Transformer │  (32, 20, 5)       # Выход трансформера
├─────────────┤
│ Output      │  (32, 20, 240)     # Логиты
├─────────────┤
│ Target      │  (32, 20)          # Целевые токены
└─────────────┘
```

### Генерация:
```
┌─────────────┐
│ Current Seq │  (1, 1) → (1, 2) → (1, 3) → ...  # Авторегрессия
├─────────────┤
│ Transformer │  (1, 1, 5) → (1, 2, 5) → ...     # Скрытые состояния
├─────────────┤
│ Logits      │  (1, 1, 240) → (1, 2, 240) → ... # Вероятности
├─────────────┤
│ Sampled     │  → 12 → 113 → 176 → ...          # Выбранные токены
└─────────────┘
```

## Ключевые Компоненты

### 1. Embedding Layer
```python
nn.Embedding(vocab_size=240, embedding_dim=5)
# 240 токенов → 5-мерные векторы
```

### 2. Positional Encoding
```python
# Добавляет позиционную информацию к эмбеддингам
# Формула: PE(pos, 2i) = sin(pos/10000^(2i/d_model))
# Формула: PE(pos, 2i+1) = cos(pos/10000^(2i/d_model))
```

### 3. Multi-Head Attention
```python
# 1 голова внимания
# Query, Key, Value: (seq_len, d_model)
# Attention: softmax(Q*K^T / sqrt(d_model)) * V
```

### 4. Feed-Forward Network
```python
# Linear(5, 20) → ReLU → Linear(20, 5)
# Residual connections + Layer Norm
```

### 5. Выходной Слой
```python
# Linear(5, 240) - проекция на словарь
```

## Параметры Модели

| Параметр | Значение | Описание |
|----------|----------|----------|
| vocab_size | 240 | Размер словаря |
| d_model | 5 | Размерность эмбеддингов |
| n_heads | 1 | Количество голов внимания |
| n_layers | 2 | Количество трансформерных слоев |
| d_ff | 20 | Размерность feed-forward сети |
| max_len | 20 | Максимальная длина последовательности |
| dropout | 0.1 | Dropout rate |

## Loss Функция

```python
# CrossEntropyLoss с игнорированием паддинга
loss = CrossEntropyLoss(ignore_index=0)  # 0 = <pad>
```

Эта диаграмма показывает полный процесс от входного текста до выходного, включая все этапы обработки в MicroTransformer.
