"""
Visualization script for the MicroTransformer process.

This script demonstrates the step-by-step process of text generation
from input to output with detailed tensor shapes and intermediate results.
"""

import sys
import os
import torch
import torch.nn.functional as F
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.tokenizer import tokenizer
from models.model import create_model

def visualize_tokenization():
    """Visualize the tokenization process."""
    print("=== 1. –¢–û–ö–ï–ù–ò–ó–ê–¶–ò–Ø ===")
    input_text = "–∞–º–±–∞ —Ö–æ–¥–∏"

    print(f"–í—Ö–æ–¥–Ω–æ–π —Ç–µ–∫—Å—Ç: '{input_text}'")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = tokenizer.tokenize(input_text)
    print(f"–¢–æ–∫–µ–Ω—ã: {tokens}")

    # –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã–º–∏ —Ç–æ–∫–µ–Ω–∞–º–∏
    encoded = tokenizer.encode(input_text, add_special_tokens=True)
    print(f"–ó–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {encoded}")
    print(f"–§–æ—Ä–º–∞ —Ç–µ–Ω–∑–æ—Ä–∞: {encoded.shape}")
    print()

def visualize_embeddings(model):
    """Visualize the embedding process."""
    print("=== 2. –≠–ú–ë–ï–î–î–ò–ù–ì–ò ===")

    # –ü—Ä–æ—Å—Ç–æ–π –ø—Ä–∏–º–µ—Ä
    test_tokens = torch.tensor([[1, 12, 113, 2, 0]], dtype=torch.long)  # [START, –∞–º–±–∞, —Ö–æ–¥–∏, END, PAD]

    print(f"–í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {test_tokens}")
    print(f"–§–æ—Ä–º–∞: {test_tokens.shape}")

    # –ü–æ–ª—É—á–∞–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
    embeddings = model.embedding(test_tokens)
    print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —Ñ–æ—Ä–º–∞: {embeddings.shape}")
    print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥ –¥–ª—è '–∞–º–±–∞' (—Ç–æ–∫–µ–Ω 12): {embeddings[0, 1]}")
    print()

def visualize_positional_encoding(model):
    """Visualize positional encoding."""
    print("=== 3. –ü–û–ó–ò–¶–ò–û–ù–ù–û–ï –ö–û–î–ò–†–û–í–ê–ù–ò–ï ===")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á
    batch_size, seq_len = 2, 5
    test_tokens = torch.randint(1, 10, (batch_size, seq_len))

    embeddings = model.embedding(test_tokens)
    pos_encoded = model.pos_encoding(embeddings)

    print(f"–¢–æ–∫–µ–Ω—ã: {test_tokens}")
    print(f"–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ + –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ —Ñ–æ—Ä–º–∞: {pos_encoded.shape}")
    print(f"–†–∞–∑–Ω–∏—Ü–∞ (–ø–µ—Ä–≤—ã–µ 3 –ø–æ–∑–∏—Ü–∏–∏): {pos_encoded[0, :3, 0] - embeddings[0, :3, 0]}")
    print()

def visualize_attention(model):
    """Visualize attention mechanism."""
    print("=== 4. –ú–ï–•–ê–ù–ò–ó–ú –í–ù–ò–ú–ê–ù–ò–Ø ===")

    # –°–æ–∑–¥–∞–µ–º —Ç–µ—Å—Ç–æ–≤—ã–π –±–∞—Ç—á
    batch_size, seq_len = 1, 4
    x = torch.randn(batch_size, seq_len, model.d_model)

    print(f"–í—Ö–æ–¥ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—É: {x.shape}")

    # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º —á–µ—Ä–µ–∑ –ø–µ—Ä–≤—ã–π —Å–ª–æ–π
    attention_output = model.layers[0].self_attn(x, x, x)[0]

    print(f"–í—ã—Ö–æ–¥ –≤–Ω–∏–º–∞–Ω–∏—è: {attention_output.shape}")
    print("–í–Ω–∏–º–∞–Ω–∏–µ –ø–æ–º–æ–≥–∞–µ—Ç –º–æ–¥–µ–ª–∏ —Ñ–æ–∫—É—Å–∏—Ä–æ–≤–∞—Ç—å—Å—è –Ω–∞ —Ä–∞–∑–Ω—ã—Ö —á–∞—Å—Ç—è—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏")
    print()

def visualize_generation(model):
    """Visualize the generation process."""
    print("=== 5. –ì–ï–ù–ï–†–ê–¶–ò–Ø –¢–ï–ö–°–¢–ê ===")

    start_token = tokenizer.vocabulary['<start>']
    print(f"–ù–∞—á–∏–Ω–∞–µ–º —Å —Ç–æ–∫–µ–Ω–∞: {start_token} ('<start>')")

    generated = [start_token]
    current_seq = torch.tensor([generated], dtype=torch.long)

    print("\n–®–∞–≥ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏:")
    for step in range(5):
        # –ü–æ–ª—É—á–∞–µ–º –≤—ã—Ö–æ–¥ –º–æ–¥–µ–ª–∏
        logits = model(current_seq)[:, -1, :]  # –ü–æ—Å–ª–µ–¥–Ω–∏–π —Ç–æ–∫–µ–Ω
        probs = F.softmax(logits, dim=-1)

        # –°–µ–º–ø–ª–∏—Ä—É–µ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–æ–∫–µ–Ω
        next_token = torch.multinomial(probs, 1).item()

        print(f"–®–∞–≥ {step+1}:")
        print(f"  –¢–µ–∫—É—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {current_seq.tolist()}")
        print(f"  –õ–æ–≥–∏—Ç—ã —Ñ–æ—Ä–º–∞: {logits.shape}")
        print(f"  –¢–æ–ø-3 –≤–µ—Ä–æ—è—Ç–Ω–æ—Å—Ç–∏: {torch.topk(probs, 3)[1].tolist()}")
        print(f"  –í—ã–±—Ä–∞–Ω —Ç–æ–∫–µ–Ω: {next_token} ({tokenizer.id_to_token.get(next_token, '<unk>')})")

        if next_token == tokenizer.vocabulary['<end>']:
            break

        generated.append(next_token)
        current_seq = torch.cat([current_seq, torch.tensor([[next_token]])], dim=1)

    # –î–µ–∫–æ–¥–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    final_text = tokenizer.decode(generated, skip_special_tokens=True)
    print(f"\n–ò—Ç–æ–≥–æ–≤—ã–π —Ç–µ–∫—Å—Ç: '{final_text}'")
    print()

def visualize_model_architecture(model):
    """Visualize model architecture."""
    print("=== –ê–†–•–ò–¢–ï–ö–¢–£–†–ê –ú–û–î–ï–õ–ò ===")

    print(f"–û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {sum(p.numel() for p in model.parameters())}")

    print("\n–°–ª–æ–∏ –º–æ–¥–µ–ª–∏:")
    for i, layer in enumerate(model.layers):
        print(f"  –¢—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–Ω—ã–π —Å–ª–æ–π {i+1}:")
        print(f"    - Self-Attention: {layer.self_attn.num_heads} –≥–æ–ª–æ–≤")
        print(f"    - Feed-Forward: {layer.feed_forward[0].out_features} –Ω–µ–π—Ä–æ–Ω–æ–≤")
        print()

    print("–†–∞–∑–º–µ—Ä—ã:")
    print(f"  - Embedding: {model.embedding.weight.shape}")
    print(f"  - –í—ã—Ö–æ–¥–Ω–æ–π —Å–ª–æ–π: {model.fc.weight.shape}")
    print()

def main():
    """Main visualization function."""
    print("üéØ –í–ò–ó–£–ê–õ–ò–ó–ê–¶–ò–Ø –ü–†–û–¶–ï–°–°–ê MicroTransformer")
    print("=" * 50)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_model()

    # –í–∏–∑—É–∞–ª–∏–∑–∏—Ä—É–µ–º —ç—Ç–∞–ø—ã
    visualize_tokenization()
    visualize_embeddings(model)
    visualize_positional_encoding(model)
    visualize_attention(model)
    visualize_generation(model)
    visualize_model_architecture(model)

    print("‚úÖ –í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")

if __name__ == "__main__":
    main()
