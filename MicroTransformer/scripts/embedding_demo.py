"""
–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏—è —Ç–æ–∫–µ–Ω–æ–≤ –≤ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏.

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ —Ç–æ–∫–µ–Ω—ã –ø—Ä–µ–æ–±—Ä–∞–∑—É—é—Ç—Å—è –≤ –≤–µ–∫—Ç–æ—Ä–Ω—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è
—Å –ø–æ–º–æ—â—å—é Embedding Layer –≤ MicroTransformer.
"""

import torch
import torch.nn as nn
import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from src.tokenizer import tokenizer
from models.model import create_model

def demonstrate_embedding_layer():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è Embedding Layer."""
    print("=== EMBEDDING LAYER –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø ===")
    print("=" * 50)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_model()

    # –ü—Ä–∏–º–µ—Ä –≤—Ö–æ–¥–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤
    example_text = "–∞–º–±–∞ —Ö–æ–¥–∏"
    print(f"–ò—Å—Ö–æ–¥–Ω—ã–π —Ç–µ–∫—Å—Ç: '{example_text}'")

    # –¢–æ–∫–µ–Ω–∏–∑–∞—Ü–∏—è
    tokens = tokenizer.tokenize(example_text)
    print(f"–¢–æ–∫–µ–Ω—ã: {tokens}")
    print(f"–°–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ —Å–ª–æ–≤–∞: {[tokenizer.id_to_token.get(t, '<unk>') for t in tokens]}")

    # –°–æ–∑–¥–∞–µ–º –±–∞—Ç—á —Ç–æ–∫–µ–Ω–æ–≤
    token_tensor = torch.tensor([tokens], dtype=torch.long)
    print(f"–§–æ—Ä–º–∞ —Ç–æ–∫–µ–Ω–æ–≤: {token_tensor.shape}")  # (batch_size=1, seq_len=2)

    # –ü—Ä–∏–º–µ–Ω—è–µ–º Embedding Layer
    embeddings = model.embedding(token_tensor)
    print(f"–§–æ—Ä–º–∞ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤: {embeddings.shape}")  # (1, 2, 5)

    print("\n–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ —Ç–æ–∫–µ–Ω–∞:")
    for i, token_id in enumerate(tokens):
        word = tokenizer.id_to_token.get(token_id, '<unk>')
        embedding = embeddings[0, i]
        print(f"  '{word}' (ID={token_id}): {embedding}")

    scaled = embeddings * 5**0.5
    print(f"\n–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: embeddings * sqrt(d_model) = {scaled}")

    return token_tensor, embeddings

def demonstrate_positional_encoding(model, token_tensor, embeddings):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è."""
    print("\n=== POSITIONAL ENCODING ===")
    print("=" * 30)

    # –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ
    pos_encoded = model.pos_encoding(embeddings)
    print(f"–§–æ—Ä–º–∞ –ø–æ—Å–ª–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–≥–æ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è: {pos_encoded.shape}")

    print("\n–î–æ–±–∞–≤–ª–µ–Ω–Ω—ã–µ –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (–ø–µ—Ä–≤—ã–µ 3 –ø–æ–∑–∏—Ü–∏–∏):")
    for pos in range(3):
        if pos < pos_encoded.shape[1]:
            pos_embedding = pos_encoded[0, pos]
            print(f"  –ü–æ–∑–∏—Ü–∏—è {pos}: {pos_embedding}")

    return pos_encoded

def demonstrate_full_forward(model, token_tensor):
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –ø–æ–ª–Ω–æ–≥–æ forward pass —á–µ—Ä–µ–∑ –º–æ–¥–µ–ª—å."""
    print("\n=== –ü–û–õ–ù–´–ô FORWARD PASS ===")
    print("=" * 30)

    print(f"–í—Ö–æ–¥–Ω—ã–µ —Ç–æ–∫–µ–Ω—ã: {token_tensor}")
    print(f"–§–æ—Ä–º–∞: {token_tensor.shape}")

    # –ü–æ–ª–Ω—ã–π forward pass
    output = model(token_tensor)
    print(f"–í—ã—Ö–æ–¥–Ω–∞—è —Ñ–æ—Ä–º–∞: {output.shape}")  # (batch_size, seq_len, vocab_size)

    print("\n–õ–æ–≥–∏—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ '–∞–º–±–∞':")
    first_token_logits = output[0, 0]  # –õ–æ–≥–∏—Ç—ã –¥–ª—è –ø–µ—Ä–≤–æ–≥–æ —Ç–æ–∫–µ–Ω–∞ –≤ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    top_5_logits, top_5_indices = torch.topk(first_token_logits, 5)

    for i in range(5):
        token_id = top_5_indices[i].item()
        word = tokenizer.id_to_token.get(token_id, '<unk>')
        logit = top_5_logits[i].item()
        print(f"  {word} (ID={token_id}): {logit:.4f}")

def demonstrate_embedding_weights():
    """–î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –≤–µ—Å–æ–≤ Embedding —Å–ª–æ—è."""
    print("\n=== –í–ï–°–ê EMBEDDING –°–õ–û–Ø ===")
    print("=" * 30)

    model = create_model()
    embedding_weights = model.embedding.weight

    print(f"–§–æ—Ä–º–∞ –≤–µ—Å–æ–≤: {embedding_weights.shape}")  # (vocab_size, d_model)
    min_val = embedding_weights.min().item()
    max_val = embedding_weights.max().item()
    print(f"–î–∏–∞–ø–∞–∑–æ–Ω –≤–µ—Å–æ–≤: [{min_val:.4f}, {max_val:.4f}]")

    # –ü–æ–∫–∞–∑–∞—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è –Ω–µ—Å–∫–æ–ª—å–∫–∏—Ö —Ç–æ–∫–µ–Ω–æ–≤
    example_tokens = [1, 12, 113, 2]  # START, –∞–º–±–∞, —Ö–æ–¥–∏, END

    print("\n–≠–º–±–µ–¥–¥–∏–Ω–≥–∏ –¥–ª—è —Å–ø–µ—Ü–∏–∞–ª—å–Ω—ã—Ö —Ç–æ–∫–µ–Ω–æ–≤:")
    for token_id in example_tokens:
        if token_id < embedding_weights.shape[0]:
            embedding = embedding_weights[token_id]
            word = tokenizer.id_to_token.get(token_id, '<unk>')
            print(f"  '{word}' (ID={token_id}): {embedding}")

def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏."""
    print("üéØ –î–ï–ú–û–ù–°–¢–†–ê–¶–ò–Ø –ü–†–ï–û–ë–†–ê–ó–û–í–ê–ù–ò–Ø –¢–û–ö–ï–ù–û–í –í –≠–ú–ë–ï–î–î–ò–ù–ì–ò")
    print("=" * 60)

    # –°–æ–∑–¥–∞–µ–º –º–æ–¥–µ–ª—å
    model = create_model()

    # –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è —à–∞–≥ –∑–∞ —à–∞–≥–æ–º
    token_tensor, embeddings = demonstrate_embedding_layer()
    pos_encoded = demonstrate_positional_encoding(model, token_tensor, embeddings)
    demonstrate_full_forward(model, token_tensor)
    demonstrate_embedding_weights()

    print("\n‚úÖ –î–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
    print("\n–ö–ª—é—á–µ–≤—ã–µ –º–æ–º–µ–Ω—Ç—ã:")
    print("- –¢–æ–∫–µ–Ω—ã ‚Üí –¶–µ–ª—ã–µ —á–∏—Å–ª–∞ (0-239)")
    print("- Embedding Layer ‚Üí –í–µ–∫—Ç–æ—Ä—ã —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç–∏ 5")
    print("- –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ: * sqrt(d_model)")
    print("- –ü–æ–∑–∏—Ü–∏–æ–Ω–Ω–æ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–æ–±–∞–≤–ª—è–µ—Ç –ø–æ–∑–∏—Ü–∏–æ–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é")

if __name__ == "__main__":
    main()
